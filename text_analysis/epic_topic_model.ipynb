{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "W-2ZFqstWNlX"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4xOX-jpX2EK0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!pip install konlpy\\n!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\\n!cd Mecab-ko-for-Google-Colab\\n!bash /content/Mecab-ko-for-Google-Colab/install_mecab-ko_on_colab_light_220429.sh\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for colab\n",
    "\"\"\"!pip install konlpy\n",
    "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
    "!cd Mecab-ko-for-Google-Colab\n",
    "!bash /content/Mecab-ko-for-Google-Colab/install_mecab-ko_on_colab_light_220429.sh\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "tagger = Mecab(dicpath=r\"C:/mecab/mecab-ko-dic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "RAcOTnxsYu2A"
   },
   "outputs": [],
   "source": [
    "yymm = \"2401\"\n",
    "\n",
    "file_path = '../data/'  # 파일 경로를 지정하세요\n",
    "fn= f'epic_metadata_{yymm}.xlsx'\n",
    "\n",
    "df = pd.read_excel(file_path+fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "id": "EPPZnSbeYyo-",
    "outputId": "970c1ab7-bb1e-4986-ab8a-f949cb6988ac"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>자료명</th>\n",
       "      <th>발간일</th>\n",
       "      <th>발간처</th>\n",
       "      <th>요약</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>246696</td>\n",
       "      <td>2024년 2월 물가연동국고채 종목별 연동계수</td>\n",
       "      <td>2024.01.02</td>\n",
       "      <td>기획재정부 국고국 국채과</td>\n",
       "      <td>기획재정부는 1.2.(화) 2024년 2월 물가연동국고채 종목별 연동계수를 발표하였...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>246697</td>\n",
       "      <td>「2024년부터 이렇게 달라집니다」 책자 발간</td>\n",
       "      <td>2023.12.28</td>\n",
       "      <td>기획재정부 기획조정실 혁신정책담당관</td>\n",
       "      <td>기획재정부는 12.31.(일) 2024년부터 달라지는 제도와 법규사항 등을 알기 쉽...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>246698</td>\n",
       "      <td>국세물납증권 56개 종목 공개매각 실시</td>\n",
       "      <td>2024.01.02</td>\n",
       "      <td>기획재정부 국고국 출자관리과</td>\n",
       "      <td>정부는 국유재산정책심의위원회에서 의결한 「2023년도 제3차 국세물납증권 매각 예정...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>246699</td>\n",
       "      <td>조달청, 1월 대형사업 총 163건, 1조 8,901억 원 상당 입찰 예정</td>\n",
       "      <td>2024.01.02</td>\n",
       "      <td>조달청</td>\n",
       "      <td>조달청은 1.2.(화) ’24년 1월 한 달 동안 총 163건 1조 8,901억 원...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>246700</td>\n",
       "      <td>인구감소지역 『생활인구』 시범산정 결과 공표</td>\n",
       "      <td>2024.01.01</td>\n",
       "      <td>통계청</td>\n",
       "      <td>통계청과 행정안전부은 1.1.(월) 7개의 인구감소지역에 대해 「생활인구」를 시범 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                        자료명         발간일  \\\n",
       "0      246696                  2024년 2월 물가연동국고채 종목별 연동계수  2024.01.02   \n",
       "1      246697                  「2024년부터 이렇게 달라집니다」 책자 발간  2023.12.28   \n",
       "2      246698                      국세물납증권 56개 종목 공개매각 실시  2024.01.02   \n",
       "3      246699  조달청, 1월 대형사업 총 163건, 1조 8,901억 원 상당 입찰 예정  2024.01.02   \n",
       "4      246700                   인구감소지역 『생활인구』 시범산정 결과 공표  2024.01.01   \n",
       "\n",
       "                   발간처                                                 요약  \n",
       "0        기획재정부 국고국 국채과  기획재정부는 1.2.(화) 2024년 2월 물가연동국고채 종목별 연동계수를 발표하였...  \n",
       "1  기획재정부 기획조정실 혁신정책담당관  기획재정부는 12.31.(일) 2024년부터 달라지는 제도와 법규사항 등을 알기 쉽...  \n",
       "2      기획재정부 국고국 출자관리과  정부는 국유재산정책심의위원회에서 의결한 「2023년도 제3차 국세물납증권 매각 예정...  \n",
       "3                  조달청  조달청은 1.2.(화) ’24년 1월 한 달 동안 총 163건 1조 8,901억 원...  \n",
       "4                  통계청  통계청과 행정안전부은 1.1.(월) 7개의 인구감소지역에 대해 「생활인구」를 시범 ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "uuKzfv1hetqX"
   },
   "outputs": [],
   "source": [
    "stop_words = \"안 간 붙임 참고 참조 첨부 총리 장관 겸 청 실 는 은 가 등 원 조 개 이 저 다만 하지만 그러나 중 억 년 월 화 수 목 금 토 일 것 줄 만 건 또한 아울러 그리고 전년 금년 이번 올해 내년 기획 재정부 첨부 파일 내용 발표 위원회 감독원 부\"\n",
    "stop_words = stop_words.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "EhtvG_7aLkEk"
   },
   "outputs": [],
   "source": [
    "excluson = [\"기획재정부\",\"정부\",'과학기술정보통신부','과기정통부','농림축산식품부','농식품부','금융위원회','금융위','금융감독원','금감원','산업통상자원부','산업부',\n",
    "            '환경부','해양수산부','해수부','공정거래위원회','공정위','식약처','식품의약품안전처','고용노동부','고용부','국토교통부','국토부','중소벤처기업부','중기부',\n",
    "            '통계청','국세청','관세청','조달청','특허청','통일부','보건복지부','복지부','교육부','한국은행',\n",
    "            '계획임.','예정임.','밝혔다.','발표하였다.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ATEQy7bM82p3"
   },
   "outputs": [],
   "source": [
    "summary_list = df['요약'].tolist()\n",
    "\n",
    "documents_list = []\n",
    "for summary in summary_list:\n",
    "    document = []\n",
    "    for ex in excluson:\n",
    "        summary = summary.replace(ex,'')\n",
    "    for noun in tagger.nouns(summary):\n",
    "        if noun not in stop_words:\n",
    "            document.append(noun)\n",
    "    documents_list.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['물가', '연동', '국고', '종목별', '연동', '계수'],\n",
       " ['제도',\n",
       "  '법규',\n",
       "  '사항',\n",
       "  '정리',\n",
       "  '책자',\n",
       "  '발간',\n",
       "  '책자',\n",
       "  '기관',\n",
       "  '정책',\n",
       "  '분야',\n",
       "  '시기',\n",
       "  '기관',\n",
       "  '구성',\n",
       "  '주요',\n",
       "  '이해',\n",
       "  '삽화',\n",
       "  '제시',\n",
       "  '청년',\n",
       "  '여성',\n",
       "  '부모',\n",
       "  '다문화가정',\n",
       "  '신혼',\n",
       "  '부부',\n",
       "  '계층',\n",
       "  '특화',\n",
       "  '정책',\n",
       "  '분야',\n",
       "  '제시',\n",
       "  '한편',\n",
       "  '사회',\n",
       "  '안전',\n",
       "  '탄소',\n",
       "  '중립',\n",
       "  '교육',\n",
       "  '지원',\n",
       "  '관련',\n",
       "  '제도',\n",
       "  '변경',\n",
       "  '다수',\n",
       "  '포함',\n",
       "  '책자',\n",
       "  '초',\n",
       "  '지방',\n",
       "  '자치',\n",
       "  '단체',\n",
       "  '공공',\n",
       "  '도서관',\n",
       "  '점자',\n",
       "  '도서관',\n",
       "  '권',\n",
       "  '배포',\n",
       "  '비치',\n",
       "  '온라인',\n",
       "  '공개',\n",
       "  '분야',\n",
       "  '주요',\n",
       "  '삽화']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_list[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [' '.join(words) for words in documents_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['물가 연동 국고 종목별 연동 계수',\n",
       " '제도 법규 사항 정리 책자 발간 책자 기관 정책 분야 시기 기관 구성 주요 이해 삽화 제시 청년 여성 부모 다문화가정 신혼 부부 계층 특화 정책 분야 제시 한편 사회 안전 탄소 중립 교육 지원 관련 제도 변경 다수 포함 책자 초 지방 자치 단체 공공 도서관 점자 도서관 권 배포 비치 온라인 공개 분야 주요 삽화']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "\n",
    "X = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1017, 1000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1017개의 문서,\n",
    "1000개의 단어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 1000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_model = TruncatedSVD(n_components=n_topics, algorithm='randomized', n_iter=100, random_state=123)\n",
    "svd_model.fit(X)\n",
    "svd_model.components_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`svd_model.components_`는 $V^T (t \\times 단어수)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: [('지원', 0.26487), ('사업', 0.24845), ('기업', 0.18016), ('기술', 0.1589), ('산업', 0.12194)]\n",
      "Topic 2: [('사업', 0.29715), ('기술', 0.27568), ('기업', 0.206), ('연구', 0.2006), ('개발', 0.19139)]\n",
      "Topic 3: [('금융', 0.29189), ('대비', 0.27157), ('증가', 0.26608), ('대출', 0.21259), ('감소', 0.19167)]\n",
      "Topic 4: [('대비', 0.20586), ('증가', 0.18783), ('기술', 0.18045), ('감소', 0.16739), ('제품', 0.16043)]\n",
      "Topic 5: [('지역', 0.2892), ('사업', 0.2349), ('대비', 0.16429), ('인구', 0.15615), ('지원', 0.15557)]\n",
      "Topic 6: [('제품', 0.29716), ('기업', 0.26778), ('수출', 0.22979), ('지원', 0.2009), ('식품', 0.18841)]\n",
      "Topic 7: [('서비스', 0.22834), ('증가', 0.19247), ('대비', 0.17882), ('정보', 0.17875), ('제품', 0.14022)]\n",
      "Topic 8: [('의료', 0.46571), ('교육', 0.27973), ('마약', 0.15343), ('창업', 0.14768), ('정책', 0.13304)]\n",
      "Topic 9: [('기업', 0.25974), ('개정', 0.24603), ('창업', 0.17741), ('개정안', 0.1755), ('시행령', 0.16618)]\n",
      "Topic 10: [('교육', 0.39035), ('안전', 0.23003), ('창업', 0.2251), ('지식', 0.12578), ('점검', 0.12563)]\n",
      "Topic 11: [('사업', 0.26381), ('의료', 0.24639), ('창업', 0.2185), ('제품', 0.19715), ('마약', 0.17201)]\n",
      "Topic 12: [('지역', 0.2769), ('인구', 0.19696), ('수출', 0.17466), ('정보', 0.1527), ('제품', 0.147)]\n",
      "Topic 13: [('서비스', 0.23636), ('청년', 0.21898), ('제품', 0.1956), ('일자리', 0.15908), ('가격', 0.14246)]\n",
      "Topic 14: [('제품', 0.34326), ('교육', 0.25804), ('지역', 0.17745), ('회수', 0.13179), ('인구', 0.12803)]\n",
      "Topic 15: [('지식', 0.15), ('식품', 0.14991), ('교육', 0.149), ('가격', 0.14655), ('재산', 0.1442)]\n",
      "Topic 16: [('보험', 0.26334), ('조사', 0.18453), ('고용', 0.1748), ('지역', 0.17057), ('보험료', 0.13669)]\n",
      "Topic 17: [('마약', 0.28565), ('기술', 0.20548), ('창업', 0.19496), ('대출', 0.15636), ('지원', 0.1496)]\n",
      "Topic 18: [('투자', 0.22757), ('청년', 0.22541), ('일자리', 0.20158), ('제품', 0.16819), ('달러', 0.16662)]\n",
      "Topic 19: [('재산', 0.2154), ('마약', 0.18183), ('지식', 0.17631), ('기술', 0.15454), ('공급', 0.14808)]\n",
      "Topic 20: [('마약', 0.27177), ('디지털', 0.17387), ('상공', 0.16098), ('공인', 0.14241), ('소상', 0.14011)]\n"
     ]
    }
   ],
   "source": [
    "# 단어 집합. 1,000개의 단어가 저장됨.\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "def get_topics(components, feature_names, n=5):\n",
    "    for idx, topic in enumerate(components):\n",
    "        print(\"Topic %d:\" % (idx+1), [(feature_names[i], topic[i].round(5)) for i in topic.argsort()[:-n - 1:-1]])\n",
    "get_topics(svd_model.components_,terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\master\\anaconda3\\envs\\py38\\lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.callbacks import CoherenceMetric\n",
    "from gensim import corpora\n",
    "from gensim.models.callbacks import PerplexityMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(documents_list)\n",
    "dictionary.filter_extremes(no_below = 5) #n회 이하로 등장한 단어는 삭제\n",
    "texts = documents_list\n",
    "corpus=[dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = n_topics\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = dictionary[0]\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average topic coherence: -4.1771.\n"
     ]
    }
   ],
   "source": [
    "top_topics = model.top_topics(corpus)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_visualization = gensimvis.prepare(model, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.save_html(lda_visualization, 'lda_epic.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KoBERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bertopic'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbertopic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BERTopic\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'bertopic'"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from bertopic import BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bertopic\n",
      "  Using cached bertopic-0.16.0-py2.py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\master\\anaconda3\\envs\\py38\\lib\\site-packages (from bertopic) (1.24.3)\n",
      "Collecting hdbscan>=0.8.29 (from bertopic)\n",
      "  Using cached hdbscan-0.8.33.tar.gz (5.2 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting umap-learn>=0.5.0 (from bertopic)\n",
      "  Using cached umap_learn-0.5.5-py3-none-any.whl\n",
      "Requirement already satisfied: pandas>=1.1.5 in c:\\users\\master\\anaconda3\\envs\\py38\\lib\\site-packages (from bertopic) (2.0.3)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2.post1 in c:\\users\\master\\anaconda3\\envs\\py38\\lib\\site-packages (from bertopic) (1.3.2)\n",
      "Requirement already satisfied: tqdm>=4.41.1 in c:\\users\\master\\anaconda3\\envs\\py38\\lib\\site-packages (from bertopic) (4.66.1)\n",
      "Collecting sentence-transformers>=0.4.1 (from bertopic)\n",
      "  Using cached sentence_transformers-2.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting plotly>=4.7.0 (from bertopic)\n",
      "  Using cached plotly-5.19.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting cython<3,>=0.27 (from hdbscan>=0.8.29->bertopic)\n",
      "  Using cached Cython-0.29.37-py2.py3-none-any.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\users\\master\\anaconda3\\envs\\py38\\lib\\site-packages (from hdbscan>=0.8.29->bertopic) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\master\\anaconda3\\envs\\py38\\lib\\site-packages (from hdbscan>=0.8.29->bertopic) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\master\\anaconda3\\envs\\py38\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\master\\anaconda3\\envs\\py38\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\master\\anaconda3\\envs\\py38\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2023.3)\n",
      "Collecting tenacity>=6.2.0 (from plotly>=4.7.0->bertopic)\n",
      "  Using cached tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\master\\anaconda3\\envs\\py38\\lib\\site-packages (from plotly>=4.7.0->bertopic) (23.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\master\\anaconda3\\envs\\py38\\lib\\site-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.2.0)\n",
      "Collecting transformers<5.0.0,>=4.32.0 (from sentence-transformers>=0.4.1->bertopic)\n",
      "  Using cached transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\master\\anaconda3\\envs\\py38\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (2.1.1)\n",
      "Collecting huggingface-hub>=0.15.1 (from sentence-transformers>=0.4.1->bertopic)\n",
      "  Using cached huggingface_hub-0.21.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: Pillow in c:\\users\\master\\anaconda3\\envs\\py38\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (10.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\master\\anaconda3\\envs\\py38\\lib\\site-packages (from tqdm>=4.41.1->bertopic) (0.4.6)\n",
      "Collecting numba>=0.51.2 (from umap-learn>=0.5.0->bertopic)\n",
      "  Using cached numba-0.58.1-cp38-cp38-win_amd64.whl.metadata (2.8 kB)\n",
      "Collecting pynndescent>=0.5 (from umap-learn>=0.5.0->bertopic)\n",
      "  Using cached pynndescent-0.5.11-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\master\\anaconda3\\envs\\py38\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\master\\anaconda3\\envs\\py38\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2023.12.1)\n",
      "Requirement already satisfied: requests in c:\\users\\master\\anaconda3\\envs\\py38\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\master\\anaconda3\\envs\\py38\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\master\\anaconda3\\envs\\py38\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (4.5.0)\n",
      "Collecting llvmlite<0.42,>=0.41.0dev0 (from numba>=0.51.2->umap-learn>=0.5.0->bertopic)\n",
      "  Using cached llvmlite-0.41.1-cp38-cp38-win_amd64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\master\\anaconda3\\envs\\py38\\lib\\site-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (6.0.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\master\\anaconda3\\envs\\py38\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.16.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\master\\anaconda3\\envs\\py38\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\master\\anaconda3\\envs\\py38\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\master\\anaconda3\\envs\\py38\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\master\\anaconda3\\envs\\py38\\lib\\site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers>=0.4.1->bertopic) (2023.10.3)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers<5.0.0,>=4.32.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Using cached tokenizers-0.15.2-cp38-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.32.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Using cached safetensors-0.4.2-cp38-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\master\\anaconda3\\envs\\py38\\lib\\site-packages (from importlib-metadata->numba>=0.51.2->umap-learn>=0.5.0->bertopic) (3.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\master\\anaconda3\\envs\\py38\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\master\\anaconda3\\envs\\py38\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\master\\anaconda3\\envs\\py38\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\master\\anaconda3\\envs\\py38\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\master\\anaconda3\\envs\\py38\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\master\\anaconda3\\envs\\py38\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n",
      "Using cached bertopic-0.16.0-py2.py3-none-any.whl (154 kB)\n",
      "Using cached plotly-5.19.0-py3-none-any.whl (15.7 MB)\n",
      "Using cached sentence_transformers-2.5.1-py3-none-any.whl (156 kB)\n",
      "Using cached Cython-0.29.37-py2.py3-none-any.whl (989 kB)\n",
      "Using cached huggingface_hub-0.21.3-py3-none-any.whl (346 kB)\n",
      "Using cached numba-0.58.1-cp38-cp38-win_amd64.whl (2.6 MB)\n",
      "Using cached pynndescent-0.5.11-py3-none-any.whl (55 kB)\n",
      "Using cached tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Using cached transformers-4.38.2-py3-none-any.whl (8.5 MB)\n",
      "Using cached llvmlite-0.41.1-cp38-cp38-win_amd64.whl (28.1 MB)\n",
      "Using cached safetensors-0.4.2-cp38-none-win_amd64.whl (268 kB)\n",
      "Using cached tokenizers-0.15.2-cp38-none-win_amd64.whl (2.2 MB)\n",
      "Building wheels for collected packages: hdbscan\n",
      "  Building wheel for hdbscan (pyproject.toml): started\n",
      "  Building wheel for hdbscan (pyproject.toml): finished with status 'error'\n",
      "Failed to build hdbscan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Building wheel for hdbscan (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [40 lines of output]\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-38\n",
      "  creating build\\lib.win-amd64-cpython-38\\hdbscan\n",
      "  copying hdbscan\\flat.py -> build\\lib.win-amd64-cpython-38\\hdbscan\n",
      "  copying hdbscan\\hdbscan_.py -> build\\lib.win-amd64-cpython-38\\hdbscan\n",
      "  copying hdbscan\\plots.py -> build\\lib.win-amd64-cpython-38\\hdbscan\n",
      "  copying hdbscan\\prediction.py -> build\\lib.win-amd64-cpython-38\\hdbscan\n",
      "  copying hdbscan\\robust_single_linkage_.py -> build\\lib.win-amd64-cpython-38\\hdbscan\n",
      "  copying hdbscan\\validity.py -> build\\lib.win-amd64-cpython-38\\hdbscan\n",
      "  copying hdbscan\\__init__.py -> build\\lib.win-amd64-cpython-38\\hdbscan\n",
      "  creating build\\lib.win-amd64-cpython-38\\hdbscan\\tests\n",
      "  copying hdbscan\\tests\\test_flat.py -> build\\lib.win-amd64-cpython-38\\hdbscan\\tests\n",
      "  copying hdbscan\\tests\\test_hdbscan.py -> build\\lib.win-amd64-cpython-38\\hdbscan\\tests\n",
      "  copying hdbscan\\tests\\test_prediction_utils.py -> build\\lib.win-amd64-cpython-38\\hdbscan\\tests\n",
      "  copying hdbscan\\tests\\test_rsl.py -> build\\lib.win-amd64-cpython-38\\hdbscan\\tests\n",
      "  copying hdbscan\\tests\\__init__.py -> build\\lib.win-amd64-cpython-38\\hdbscan\\tests\n",
      "  running build_ext\n",
      "  cythoning hdbscan/_hdbscan_tree.pyx to hdbscan\\_hdbscan_tree.c\n",
      "  cythoning hdbscan/_hdbscan_linkage.pyx to hdbscan\\_hdbscan_linkage.c\n",
      "  cythoning hdbscan/_hdbscan_boruvka.pyx to hdbscan\\_hdbscan_boruvka.c\n",
      "  cythoning hdbscan/_hdbscan_reachability.pyx to hdbscan\\_hdbscan_reachability.c\n",
      "  cythoning hdbscan/_prediction_utils.pyx to hdbscan\\_prediction_utils.c\n",
      "  cythoning hdbscan/dist_metrics.pyx to hdbscan\\dist_metrics.c\n",
      "  building 'hdbscan._hdbscan_tree' extension\n",
      "  C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\pip-build-env-ncbe4271\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\pip-install-b32q72zh\\hdbscan_25f9ed3bc58540569c21b7a24faca55a\\hdbscan\\_hdbscan_tree.pyx\n",
      "    tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "  C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\pip-build-env-ncbe4271\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\pip-install-b32q72zh\\hdbscan_25f9ed3bc58540569c21b7a24faca55a\\hdbscan\\_hdbscan_linkage.pyx\n",
      "    tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "  C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\pip-build-env-ncbe4271\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\pip-install-b32q72zh\\hdbscan_25f9ed3bc58540569c21b7a24faca55a\\hdbscan\\_hdbscan_boruvka.pyx\n",
      "    tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "  C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\pip-build-env-ncbe4271\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\pip-install-b32q72zh\\hdbscan_25f9ed3bc58540569c21b7a24faca55a\\hdbscan\\_hdbscan_reachability.pyx\n",
      "    tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "  C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\pip-build-env-ncbe4271\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\pip-install-b32q72zh\\hdbscan_25f9ed3bc58540569c21b7a24faca55a\\hdbscan\\_prediction_utils.pyx\n",
      "    tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "  C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\pip-build-env-ncbe4271\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\pip-install-b32q72zh\\hdbscan_25f9ed3bc58540569c21b7a24faca55a\\hdbscan\\dist_metrics.pxd\n",
      "    tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for hdbscan\n",
      "ERROR: Could not build wheels for hdbscan, which is required to install pyproject.toml-based projects\n"
     ]
    }
   ],
   "source": [
    "!pip install bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
